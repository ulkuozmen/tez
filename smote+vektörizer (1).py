#!/usr/bin/env python
# coding: utf-8

# Geleneksel: TF-IDF / CountVectorizer / Hashing Vectorizer
# 
# Derin Ã–ÄŸrenme: BERT vektÃ¶rleri
# 
# Klasik ML Modelleri:
# â†’ Naive Bayes
# â†’ Random Forest
# â†’ Logistic Regression
# â†’ SVM
# 
# Derin Ã–ÄŸrenme Modelleri:
# â†’ BERT + LSTM
# â†’ ANN (Artificial Neural Network)
# 
# 	KarÅŸÄ±laÅŸtÄ±rma: Accuracy / Precision / Recall / F1 Score ile tÃ¼m modelleri karÅŸÄ±laÅŸtÄ±rmak

# In[17]:


import pandas as pd
import seaborn as sns
import re
import nltk
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from transformers import BertTokenizer, BertModel
import torch
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import warnings
warnings.simplefilter("ignore")
from urllib.parse import urlparse
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE


# In[18]:


# Veri Setini YÃ¼kleme
file_path = "phishing_site_urls.csv"  # DosyanÄ±n bulunduÄŸu yolu belirtin
df = pd.read_csv(file_path)


# In[19]:


df.info()


# In[20]:


print("\n Veri seti boyutu:")
print(df.shape)


# In[21]:


# Eksik deÄŸer kontrolÃ¼
print("\n Eksik DeÄŸer SayÄ±sÄ±:")
print(df.isnull().sum())


# In[22]:


# Etiket (Label) daÄŸÄ±lÄ±mÄ±
print("\n Etiket DaÄŸÄ±lÄ±mÄ±:")
print(df['Label'].value_counts())


# In[23]:


# Yinelenen satÄ±r sayÄ±sÄ±nÄ± bul
yinelenen_sayi = df.duplicated().sum()
print(f" Yinelenen kayÄ±t sayÄ±sÄ±: {yinelenen_sayi}")


# In[24]:


df = df.drop_duplicates()
print(f" Yinelenen satÄ±rlar temizlendi. Yeni veri seti boyutu: {df.shape}")


# In[25]:


print("Etiket daÄŸÄ±lÄ±mÄ± (Label):")
print(df['Label'].value_counts())


# In[26]:


# Kategorik DeÄŸerlerin Ä°ÅŸlenmesi
df['Label'] = df['Label'].map({'good': 1, 'bad': 0})

print(df.head())


# In[27]:


# Etiket daÄŸÄ±lÄ±mÄ±
plt.figure(figsize=(6,4))
sns.countplot(x=df['Label'], palette=['red', 'green'])
plt.title('Etiket DaÄŸÄ±lÄ±mÄ± ("Good" ve "Bad" URL\'ler)')
plt.xlabel('Etiket')
plt.ylabel('Adet')
plt.xticks([0, 1], ["Bad", "Good"])
plt.show()


# In[28]:


X = df['URL']  # URL'ler
y = df['Label']  # Etiketler


# In[29]:


#  EÄŸitim-Test AyrÄ±mÄ±
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)


# In[30]:


from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer

# VektÃ¶rizerler
vectorizers = {
    "CountVectorizer": CountVectorizer(stop_words='english', max_features=5000),
    "TF-IDF": TfidfVectorizer(stop_words='english', max_features=5000),
    "HashingVectorizer": HashingVectorizer(n_features=5000, alternate_sign=False)
}


# In[31]:


from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# Modeller
models = {
    "Naive Bayes": MultinomialNB(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
   # "SVM": SVC(kernel='linear', random_state=42)
    "KNN": KNeighborsClassifier(n_neighbors=5)  #varsayÄ±lan 5 komÅŸu
}
# SonuÃ§larÄ± SaklayacaÄŸÄ±mÄ±z Liste
results = []


# In[32]:


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

for vect_name, vect in vectorizers.items():
    print(f"\nğŸ”µ VektÃ¶rizer: {vect_name}")

    # EÄŸitim ve Test VektÃ¶rlerini Ã§Ä±kar
    X_train_vect = vect.fit_transform(X_train)
    X_test_vect = vect.transform(X_test)

    # SMOTE uygulamasÄ±
    smote = SMOTE(random_state=42)
    X_train_smote, y_train_smote = smote.fit_resample(X_train_vect, y_train)

    # MODELLER
    for model_name, model in models.items():
        print(f"  ğŸ”¸ Model: {model_name}")

        # Modeli EÄŸit
        model.fit(X_train_smote, y_train_smote)

        # Test verisi Ã¼zerinde tahmin
        y_pred = model.predict(X_test_vect)

        # Performans Metrikleri
        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred)
        rec = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        # SonuÃ§larÄ± kaydet
        results.append({
            "Vectorizer": vect_name,
            "Model": model_name,
            "Accuracy": acc,
            "Precision": prec,
            "Recall": rec,
            "F1-Score": f1
        })


# In[33]:


print("SMOTE sonrasÄ± eÄŸitim veri seti boyutu:", X_train_smote.shape)
print("SMOTE sonrasÄ± etiket daÄŸÄ±lÄ±mÄ±:", np.bincount(y_train_smote))


# In[34]:


from tabulate import tabulate
results_df = pd.DataFrame(results)

print("\nâœ… TÃ¼m SonuÃ§lar:")
print(tabulate(results_df, headers='keys', tablefmt='pretty'))


# In[36]:


# En iyi sonucu bul
best_result = sorted_results.iloc[0]

print("\nğŸ† En Ä°yi Model:")
print(f"VektÃ¶rizer: {best_result['Vectorizer']}")
print(f"Model: {best_result['Model']}")
print(f"Accuracy: {best_result['Accuracy']:.4f}")
print(f"F1-Score: {best_result['F1-Score']:.4f}")


# In[66]:


import joblib

# En iyi vektÃ¶rizer ve model nesnesini kaydet 
joblib.dump(vect, 'hashing_vectorizer.pkl')
joblib.dump(model, 'random_forest_model.pkl')


# In[38]:


# 1. SonuÃ§larÄ± sÄ±ralama
sorted_results = results_df.sort_values(by="Accuracy", ascending=False)

# 2. En iyi model
best_result = sorted_results.iloc[0]

print("\nğŸ† En Ä°yi Model:")
print(f"VektÃ¶rizer: {best_result['Vectorizer']}")
print(f"Model: {best_result['Model']}")
print(f"Accuracy: {best_result['Accuracy']:.4f}")
print(f"F1-Score: {best_result['F1-Score']:.4f}")

# 3. Accuracy'leri bar chart ile gÃ¶sterelim
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.barh(
    sorted_results['Vectorizer'] + " + " + sorted_results['Model'],
    sorted_results['Accuracy'],
    color='skyblue'
)
plt.xlabel('Accuracy')
plt.title('Model BaÅŸarÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±')
plt.gca().invert_yaxis()
plt.grid(axis='x')
plt.show()


# In[43]:


get_ipython().system('pip install -q sentence-transformers')
from sentence_transformers import SentenceTransformer
# BERT modeli yÃ¼kle
bert_model = SentenceTransformer('all-MiniLM-L6-v2')

# URL'leri kÃ¼Ã§Ã¼k harfe Ã§evir (temizleme)
X = df['URL'].str.lower()

# BERT ile embedding Ã§Ä±kar
X_embeddings = bert_model.encode(X.tolist(), show_progress_bar=True)

# SonuÃ§lar: NumPy Array
print("BERT embedding boyutu:", X_embeddings.shape)


# BERT Ã¶zelliÄŸi Ã§Ä±karÄ±ldÄ±, metinler artÄ±k sayÄ±sal vektÃ¶r oldu.

# In[46]:


from sklearn.model_selection import train_test_split

y = df['Label'].values  # 0 = bad, 1 = good

X_train, X_test, y_train, y_test = train_test_split(
    X_embeddings, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

print("EÄŸitim seti boyutu:", X_train.shape)
print("Test seti boyutu:", X_test.shape)



# In[47]:


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

model = Sequential([
    Dense(256, activation='relu', input_shape=(384,)),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()


# ANN modelin giriÅŸ katmanÄ±nda input_shape=(384,) yazÄ±yor.
# 
# Bu 384 boyut, BERT modelinden Ã§Ä±kan embedding boyutu.
# 
# Yani doÄŸrudan BERT'in Ã¼rettiÄŸi embeddingler ANN giriÅŸine verildi.

# In[48]:


early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=50,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

BERT embeddingleri Ã¼stÃ¼nde SMOTE yaparsak model anlamlÄ± yerlerde deÄŸil, bozulmuÅŸ yapay yerlerde Ã¶ÄŸrenir.

monitor='val_loss'	Her epoch sonunda Validation Loss deÄŸerini takip eder.
patience=5	Validation Loss Ã¼st Ã¼ste 5 epoch boyunca iyileÅŸmezse eÄŸitimi durdurur.
restore_best_weights=True	EÄŸitimi durdurduktan sonra, en iyi (en dÃ¼ÅŸÃ¼k val_loss) olan aÄŸÄ±rlÄ±klarÄ± geri yÃ¼kler. BÃ¶ylece model overfitting yapmadan en iyi halini otomatik alÄ±r. 
# Kaggle'dan aldÄ±ÄŸÄ±m URL'leri SentenceTransformer modeli (all-MiniLM-L6-v2) ile embedledim. Her URL â” 384 boyutlu bir vektÃ¶r oldu.
# 
# BERT embedding'lerini eÄŸitim ve test setine ayÄ±rdÄ±m.
# 
# 
# - GiriÅŸ: 384 boyutlu BERT embedding
# - Dense katmanlar: 256 â” 128 â” 64 nÃ¶ron
# - Ã‡Ä±kÄ±ÅŸ: 1 nÃ¶ron (sigmoid aktivasyon, binary classification)
# 

# In[49]:


max_train_acc = max(history.history['accuracy'])
print(f" EÄŸitim (Train) Setinde En YÃ¼ksek DoÄŸruluk: {max_train_acc:.4f}")
max_val_acc = max(history.history['val_accuracy'])
print(f" DoÄŸrulama (Validation) Setinde En YÃ¼ksek DoÄŸruluk: {max_val_acc:.4f}")


# In[50]:


# Loss GrafiÄŸi
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss GrafiÄŸi')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid()

# Accuracy GrafiÄŸi
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy GrafiÄŸi')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()

plt.tight_layout()
plt.show()


# In[64]:


model.save("bert_ann_model.h5")


# In[58]:


X_embeddings_lstm = np.expand_dims(X_embeddings, axis=-1)


# In[59]:


X_train, X_test, y_train, y_test = train_test_split(
    X_embeddings_lstm, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)


# In[60]:


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

model_lstm = Sequential([
    LSTM(128, input_shape=(384,1), return_sequences=False),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

model_lstm.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model_lstm.summary()


# LSTM(128)	BERT embedding Ã¼zerinden 128 adet gizli durum (hidden state) Ã§Ä±karÄ±yor
# 
# Dropout(0.3)	%30 dropout â” overfitting Ã¶nlemek iÃ§in
# 
# Dense(64)	64 nÃ¶ronlu fully connected katman
# 
# Dropout(0.3)	Tekrar dropout â” daha gÃ¼Ã§lÃ¼ regularization
# 
# Dense(1)	1 nÃ¶ron (sigmoid aktivasyon) â” binary sÄ±nÄ±flandÄ±rma (good vs bad URL)

# In[61]:


early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

history_lstm = model_lstm.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=50,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)


# Batch Size = 64 â” her adÄ±mda 64 Ã¶rnek iÅŸleniyor.

# In[62]:


print(f"ğŸ“ˆ En yÃ¼ksek validation doÄŸruluÄŸu: {max(history_lstm.history['val_accuracy']):.4f}")


# In[63]:


import matplotlib.pyplot as plt

# Grafik boyutu
plt.figure(figsize=(14, 6))

# 1. Loss GrafiÄŸi
plt.subplot(1, 2, 1)
plt.plot(history_lstm.history['loss'], label='Train Loss')
plt.plot(history_lstm.history['val_loss'], label='Validation Loss')
plt.title('Loss GrafiÄŸi (BERT + LSTM)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid()

# 2. Accuracy GrafiÄŸi
plt.subplot(1, 2, 2)
plt.plot(history_lstm.history['accuracy'], label='Train Accuracy')
plt.plot(history_lstm.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy GrafiÄŸi (BERT + LSTM)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()

# TÃ¼m grafikleri hizala
plt.tight_layout()

# GÃ¶ster
plt.show()


# In[65]:


model_lstm.save('bert_lstm_model.h5')


# In[ ]:




